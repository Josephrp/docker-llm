services:
  server:
    build:
      context: .
    container_name: llm-server
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
              count: all
              driver: nvidia
    environment:
      HF_TOKEN: $HF_TOKEN
      MODEL: "mistralai/Mistral-7B-v0.1"
      TENSOR_PARALLEL_SIZE: "1"
    image: ivangabriele/llm:base
    ipc: host
    logging:
      driver: json-file
      options:
        max-file: "1"
        max-size: "10m"
    ports:
      - 8000:8000
    shm_size: 1024M
    ulimits:
      stack: 67108864
