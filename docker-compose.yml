services:
  server:
    build:
      args:
        HF_TOKEN: $HF_TOKEN
        MAX_MODEL_LENGTH: "16314"
        MODEL: "Open-Orca/LlongOrca-13B-16k"
      context: .
    container_name: llm-server
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
              count: all
              driver: nvidia
    environment:
      TENSOR_PARALLEL_SIZE: "1"
    image: ivangabriele/llm:open-orca__llongorca-13b-16k
    ipc: host
    logging:
      driver: json-file
      options:
        max-file: "1"
        max-size: "10m"
    ports:
      - "22:22"
      - "8000:8000"
    shm_size: 1024M
    ulimits:
      stack: 67108864
